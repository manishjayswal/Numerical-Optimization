{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem Set - 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv-xwYNlYU1y"
      },
      "source": [
        "[link text](https://)# DS-211 : Aug-2021 : PS-2\n",
        "\n",
        "## **Question : 1** \n",
        "\n",
        "\n",
        "**Steepest Descent and Newton's Line Search Methods**\n",
        "\n",
        "Consider the following quadratic functions:\n",
        "1. $f_1(x) = \\frac{1}{2} x^T A_1 x $\n",
        "  \n",
        "  where \n",
        "$A_1 = \n",
        "  \\begin{pmatrix}\n",
        "  1 & 0 \\\\\n",
        "  0 & 1 \\\\\n",
        "  \\end{pmatrix}$\n",
        "\n",
        "2. $f_2(x) = \\frac{1}{2} x^T A_2 x $\n",
        "  \n",
        "  where \n",
        "$A_2 = \n",
        "  \\begin{pmatrix}\n",
        "  10 & 8 \\\\\n",
        "  8 & 10 \\\\\n",
        "  \\end{pmatrix}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6rWpYZ4UIKl"
      },
      "source": [
        "\n",
        "\n",
        "1. Find the minima $x^*$ for the given functions $f_1(x)$ and  $f_2(x)$ using your own implementation of Steepest Descent. Compute the step lenght by implementing the backtracking algorithm (Algorithm 3.1 Nocedal and Wright) with $\\rho = 0.9$ and $c = 10^{-4}$. **[1.5 Points]**\n",
        "\n",
        "2. Find the minima $x^*$ for the given functions $f_1(x)$ and  $f_2(x)$ using your own implementation of Newton's Method. **[1 Point]**\n",
        "\n",
        "Notes:\n",
        "1. Run both algorithms for two initial guesses. i. $x_0=(2,0)$ and ii. $x_0=(2,2)$\n",
        "2. Stop iterations when $||x_{k+1} - x_{k}||_2^2 < 10^{-5}$\n",
        "3. For each case report the solution and the number of iterations to converge. Also comment on the reported number of iterations.\n",
        "4. Show the function contour plot and the iterates {$x_k$} including the solution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLjXDIayjo47"
      },
      "source": [
        "# **Question : 2**\n",
        "\n",
        "**Stochastic Gradient Descent**\n",
        "\n",
        "Download the data [here](https://drive.google.com/file/d/19H5bjePCXkEoaeoseDh6SymjOuJVFMqH/view?usp=sharing) $(X, Y)$ for a linear regreassion problem, where $X = [x_1, x_2, ..., x_n]^T$ and $Y = [y_1, y_2, ... , y_n]^T$. You must find the best fit for the model $Y=pX+q$ where $p$ and $q$ are scalars.\n",
        "\n",
        "1. Formulate the quadratic loss function for the Mean Squared Error of the general form $\\frac{1}{2} z^T A z  + b^T z + c$, where $z = [p, q]^T$. Write $A$ and $b$ in terms of $X$ and $Y$. Also re-write the same in terms of $x_i, y_i, p$ and $q$. **[0.5 Points]**\n",
        "\n",
        "2. Implement the vanilla Stochastic Gradient Descent algorithm (sample $1$ without replacement out of the $n$ gradients) to minimize the forumulated loss function. Experiment with the step lengths of 0.01 and 0.1 (Keep step lengths constant across iterations). Use initial guess as $x_0 = [0,0]^T$ **[1 Points]**\n",
        "\n",
        "3. Compare results with your Gradient Descent implementation. **[0.5 Points]**\n",
        "\n",
        "4. Solve the above problem by using scikit-learn's implementation of stochastic gradient descent named sklearn.linear_model.SGDRegressor. [Click here for documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor). **[0.5 Points]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9RuBYMTsxzu"
      },
      "source": [
        "Notes:\n",
        "1. Stop iterations when $||x_{k+1} - x_{k}||_2^2 < 10^{-5}$\n",
        "3. For each case report the solution and the number of iterations to converge. Comment on the number of iterations. \n",
        "4. Show the function contour plot and the iterates {$x_k$} including the solution for questions Q2.2 and Q2.3. For Q2.2 comment on the iterate sequence obtained for the two step-lengths.\n",
        "4. For Q2.4, make a scatter plot for given data $X$ vs $Y$. Let $Y\\_pred$ be the model prediction on the same $X$. Make a scatter plot for $X$ vs $Y\\_pred$ in the same figure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIkxOAtEs_82"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}